{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3068 with penalties: 1002\n",
      "Timesteps taken: 891 with penalties: 277\n",
      "Timesteps taken: 514 with penalties: 174\n",
      "Timesteps taken: 6100 with penalties: 2034\n",
      "Timesteps taken: 3539 with penalties: 1175\n",
      "Timesteps taken: 2645 with penalties: 896\n",
      "Timesteps taken: 2572 with penalties: 855\n",
      "Timesteps taken: 314 with penalties: 113\n",
      "Timesteps taken: 1849 with penalties: 573\n",
      "Timesteps taken: 915 with penalties: 316\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Random policy ----------------------------------\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "for i in range(10):   \n",
    "    env.reset()\n",
    "    epochs = 0\n",
    "    penalties, reward = 0,0\n",
    "    frames = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # action_space.sample() returns a random action\n",
    "        action = env.action_space.sample()\n",
    "        # play the random action\n",
    "        state, reward, done, info = env.step(action)    \n",
    "        if reward == -10:\n",
    "            penalties += 1        \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )    \n",
    "        epochs += 1\n",
    "    print(\"Timesteps taken: {} with penalties: {}\".format(epochs, penalties))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "env.reset()\n",
    "\n",
    "# Q Learning ---------------------------\n",
    "\n",
    "# 500 x 6 table for storing Q values\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "# Train agent for 100000 episodes, currently takes 1 minute to complete\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #Assume that with low epsilon a random action will be selected less often\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            # Select the highest weighted action from the Q Table\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)         \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])   \n",
    "        \n",
    "        # Q Learning algorithm\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, action] = new_value\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 14 with Penalties incurred: 0\n",
      "Timesteps taken: 13 with Penalties incurred: 0\n",
      "Timesteps taken: 11 with Penalties incurred: 0\n",
      "Timesteps taken: 11 with Penalties incurred: 0\n",
      "Timesteps taken: 12 with Penalties incurred: 0\n",
      "Timesteps taken: 12 with Penalties incurred: 0\n",
      "Timesteps taken: 13 with Penalties incurred: 0\n",
      "Timesteps taken: 12 with Penalties incurred: 0\n",
      "Timesteps taken: 15 with Penalties incurred: 0\n",
      "Timesteps taken: 10 with Penalties incurred: 0\n",
      "Results after 10 episodes:\n",
      "Average rewards per move: 1.6260162601626016\n",
      "Average timesteps per episode: 12.3\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_rewards += reward\n",
    "    print(\"Timesteps taken: {} with Penalties incurred: {}\".format(epochs, penalties))\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average rewards per move: {total_rewards / total_epochs}\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell's contents are sourced from: \n",
    "# https://github.com/allanbreyes/gym-solutions/blob/master/analysis/mdp.py\n",
    "\n",
    "#These functions are to perform Temporal Difference Learning of the Taxi problem\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        #print '%s function took %0.3f ms' % (f.func_name, (time2-time1)*1000.0)\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "def policy_iteration(problem, R=None, T=None, gamma=0.9, max_iterations=10**6, delta=10**-3):\n",
    "    \"\"\" Runs Policy Iteration on a gym problem \"\"\"\n",
    "    num_spaces = problem.observation_space.n\n",
    "    num_actions = problem.action_space.n\n",
    "\n",
    "    # Initialize with a random policy and initial value function\n",
    "    policy = np.array([problem.action_space.sample() for _ in range(num_spaces)])\n",
    "    value_fn = np.zeros(num_spaces)\n",
    "\n",
    "    # Get transitions and rewards\n",
    "    if R is None or T is None:\n",
    "        R, T = evaluate_rewards_and_transitions(problem)\n",
    "\n",
    "    # Iterate and improve policies\n",
    "    for i in range(max_iterations):\n",
    "        previous_policy = policy.copy()\n",
    "\n",
    "        for j in range(max_iterations):\n",
    "            previous_value_fn = value_fn.copy()\n",
    "            Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "            value_fn = np.sum(encode_policy(policy, (num_spaces, num_actions)) * Q, 1)\n",
    "\n",
    "            if np.max(np.abs(previous_value_fn - value_fn)) < delta:\n",
    "                break\n",
    "\n",
    "        Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "        policy = np.argmax(Q, axis=1)\n",
    "\n",
    "        if np.array_equal(policy, previous_policy):\n",
    "            break\n",
    "\n",
    "    # Return optimal policy\n",
    "    return policy, i + 1\n",
    "\n",
    "def value_iteration(problem, R=None, T=None, gamma=0.9, max_iterations=10**6, delta=10**-3):\n",
    "    \"\"\" Runs Value Iteration on a gym problem \"\"\"\n",
    "    value_fn = np.zeros(problem.observation_space.n)\n",
    "    if R is None or T is None:\n",
    "        R, T = evaluate_rewards_and_transitions(problem)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        previous_value_fn = value_fn.copy()\n",
    "        Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "        value_fn = np.max(Q, axis=1)\n",
    "\n",
    "        if np.max(np.abs(value_fn - previous_value_fn)) < delta:\n",
    "            break\n",
    "\n",
    "    # Get and return optimal policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy, i + 1\n",
    "\n",
    "def evaluate_rewards_and_transitions(problem, mutate=False):\n",
    "    # Enumerate state and action space sizes\n",
    "    num_states = problem.observation_space.n\n",
    "    num_actions = problem.action_space.n\n",
    "\n",
    "    # Intiailize T and R matrices\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    T = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "    # Iterate over states, actions, and transitions\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            for transition in problem.env.P[state][action]:\n",
    "                probability, next_state, reward, done = transition\n",
    "                R[state, action, next_state] = reward\n",
    "                T[state, action, next_state] = probability\n",
    "\n",
    "            # Normalize T across state + action axes\n",
    "            T[state, action, :] /= np.sum(T[state, action, :])\n",
    "\n",
    "    # Conditionally mutate and return\n",
    "    if mutate:\n",
    "        problem.env.R = R\n",
    "        problem.env.T = T\n",
    "    return R, T\n",
    "\n",
    "def encode_policy(policy, shape):\n",
    "    \"\"\" One-hot encodes a policy \"\"\"\n",
    "    encoded_policy = np.zeros(shape)\n",
    "    encoded_policy[np.arange(shape[0]), policy] = 1\n",
    "    return encoded_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD Learning -----------------------------------------------\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "# Calling value_itertation() sets the values in the value_policy table.\n",
    "# The value_policy table is used in the next step for path traversal\n",
    "value_policy, iters = value_iteration(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 17 with Penalties incurred: 0\n",
      "Timesteps taken: 11 with Penalties incurred: 0\n",
      "Timesteps taken: 15 with Penalties incurred: 0\n",
      "Timesteps taken: 13 with Penalties incurred: 0\n",
      "Timesteps taken: 14 with Penalties incurred: 0\n",
      "Timesteps taken: 12 with Penalties incurred: 0\n",
      "Timesteps taken: 16 with Penalties incurred: 0\n",
      "Timesteps taken: 13 with Penalties incurred: 0\n",
      "Timesteps taken: 16 with Penalties incurred: 0\n",
      "Timesteps taken: 14 with Penalties incurred: 0\n",
      "Results after 10 episodes:\n",
      "Average rewards per move: 1.4184397163120568\n",
      "Average timesteps per episode: 14.1\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after TD Learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get the action as dictated in the value_policy table\n",
    "        action = value_policy[state]\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    print(\"Timesteps taken: {} with Penalties incurred: {}\".format(epochs, penalties))\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_rewards += reward\n",
    "    \n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average rewards per move: {total_rewards / total_epochs}\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
